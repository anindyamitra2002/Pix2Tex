{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.27.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage - Memory usage: 0.17 GB\n",
      "\n",
      "Processing dataset 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433842826efb4aef83e18bdbddd4eef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/76266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing dataset 1 - Memory usage: 10.49 GB\n",
      "\n",
      "Processing dataset 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb2d3f8c77d49bf813932e488659473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/552340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing dataset 2 - Memory usage: 59.33 GB\n",
      "\n",
      "Combining datasets...\n",
      "After combining datasets - Memory usage: 59.33 GB\n",
      "\n",
      "Creating and processing splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27321cd894344a91aeaf69a6da6df3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After creating splits - Memory usage: 63.56 GB\n",
      "\n",
      "Pushing to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c002d353e8e04b80b32342959e8e3233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a43491cc864b1a8f8c6e9b9dcf5357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfb0199554a458bba7cbf42284e703f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/943 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b6288ed0f14c508424e20b87173b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f77f49fe0748d3ada48e3c4535e374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/943 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcadfeb5c8644af2b90b0571b3e6be2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784849803353468ab33ba247e0c982e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/943 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe7e49f6f754dbe86aaed2f98995af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0e039def78424cb81a3f4cadcf396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/943 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e263df8b7c4929878a8c9c1f532e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bd797a50ab42bd837fc666b2c053f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e4372fa12a4de0b64bbc1b903324b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003d1086daf44a2bb26c4f3ee0f63999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3227f52bd26430d8b70a7975b25ca78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b70bcaa1c747a8a2748b2087d1ca08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff3c2048caa4f4c9a0ce060e982aba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c4c61ba94844de9d49f59dcceebd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40db2fb49514476b84af1f4a79295df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7c7b06929b4f88a3205222b82b5e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pushing to hub - Memory usage: 64.46 GB\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024 / 1024  # Convert bytes to GB\n",
    "\n",
    "def log_memory(message):\n",
    "    \"\"\"Log memory usage with a message\"\"\"\n",
    "    print(f\"{message} - Memory usage: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "def process_dataset(dataset, column_name, chunk_size=1000):\n",
    "    \"\"\"Process a single dataset in chunks with progress bar\"\"\"\n",
    "    total_len = len(dataset)\n",
    "    all_images = []\n",
    "    all_latex = []\n",
    "    \n",
    "    # Create chunks of indices\n",
    "    chunks = range(0, total_len, chunk_size)\n",
    "    \n",
    "    # Process chunks with progress bar\n",
    "    with tqdm(total=total_len, desc=\"Processing dataset\") as pbar:\n",
    "        for i in chunks:\n",
    "            # Get chunk indices\n",
    "            end_idx = min(i + chunk_size, total_len)\n",
    "            chunk = dataset.select(range(i, end_idx))\n",
    "            \n",
    "            # Append data\n",
    "            all_images.extend(chunk['image'])\n",
    "            all_latex.extend(chunk[column_name])\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - i)\n",
    "            \n",
    "            # Clear memory\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "    \n",
    "    return all_images, all_latex\n",
    "\n",
    "def create_splits(images, latex, train_size=0.6, val_size=0.2, test_size=0.2):\n",
    "    \"\"\"Create dataset splits with progress bar\"\"\"\n",
    "    total_samples = len(images)\n",
    "    \n",
    "    with tqdm(total=3, desc=\"Creating splits\") as pbar:\n",
    "        # Create train/test split\n",
    "        train_val_size = train_size + val_size\n",
    "        train_val_images, test_images, train_val_latex, test_latex = train_test_split(\n",
    "            images, latex, test_size=test_size, random_state=42\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Create train/val split\n",
    "        val_size_adjusted = val_size / train_val_size\n",
    "        train_images, val_images, train_latex, val_latex = train_test_split(\n",
    "            train_val_images, train_val_latex, test_size=val_size_adjusted, random_state=42\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Create datasets\n",
    "        splits = DatasetDict({\n",
    "            'train': Dataset.from_dict({'image': train_images, 'latex': train_latex}),\n",
    "            'validation': Dataset.from_dict({'image': val_images, 'latex': val_latex}),\n",
    "            'test': Dataset.from_dict({'image': test_images, 'latex': test_latex})\n",
    "        })\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def main():\n",
    "    chunk_size = 20000  # Adjust based on your memory constraints\n",
    "    \n",
    "    log_memory(\"Initial memory usage\")\n",
    "    \n",
    "    # Load and process first dataset\n",
    "    print(\"\\nProcessing dataset 1...\")\n",
    "    dataset1 = load_dataset(\"linxy/LaTeX_OCR\", name=\"synthetic_handwrite\", split=\"train\")\n",
    "    images1, latex1 = process_dataset(dataset1, 'text', chunk_size)\n",
    "    del dataset1\n",
    "    gc.collect()\n",
    "    log_memory(\"After processing dataset 1\")\n",
    "    \n",
    "    # Load and process second dataset\n",
    "    print(\"\\nProcessing dataset 2...\")\n",
    "    dataset2 = load_dataset(\"OleehyO/latex-formulas\", \"cleaned_formulas\", split=\"train\")\n",
    "    images2, latex2 = process_dataset(dataset2, 'latex_formula', chunk_size)\n",
    "    del dataset2\n",
    "    gc.collect()\n",
    "    log_memory(\"After processing dataset 2\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    print(\"\\nCombining datasets...\")\n",
    "    all_images = images1 + images2\n",
    "    all_latex = latex1 + latex2\n",
    "    del images1, images2, latex1, latex2\n",
    "    gc.collect()\n",
    "    log_memory(\"After combining datasets\")\n",
    "    \n",
    "    # Create splits\n",
    "    print(\"\\nCreating and processing splits...\")\n",
    "    splits = create_splits(all_images, all_latex)\n",
    "    log_memory(\"After creating splits\")\n",
    "    \n",
    "    # Push to hub with progress tracking\n",
    "    print(\"\\nPushing to Hugging Face Hub...\")\n",
    "    splits.push_to_hub(\"anindya-hf-2002/pix2tex\", private=True)\n",
    "    log_memory(\"After pushing to hub\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from anindya-hf-2002/pix2tex_processed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3018f8d61d84049b0ca41e6e8e6b66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/537 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1511052fda1c45618c83d4dc3c01d6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/34.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a9c9dafe8f489293817cbc3df3b1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9126aef9fa1f41c0b56f45a19c4fa216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30e5d24924542e681965b48e5b02107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/377163 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81c3614be694d24aadff96ee2eeaa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/125721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339adf3c645544c9b066baa59d12c4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/125722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f63a2bd01049998b992bcb6cb1add8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train paths:   0%|          | 0/377163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a25a4cf7ea463d9d7ed345929753ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation paths:   0%|          | 0/125721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb006e3869c74aa9928c131cb1ca49c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test paths:   0%|          | 0/125722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing processed dataset to anindya-hf-2002/pix2tex_data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde2ad9850ed4005b8f8cde303fc0340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f0ed9cd23e4035b9ead78a5bd5a50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/378 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b73f58dc16e40cc90e1d50521268a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cc60814d0e4a2fbeebc06e0180e3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/126 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ad373171fd4714b803eaf51f28199e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b0b9000a1b4ad7bb85c787dbf36441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/126 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed to HuggingFace!\n",
      "\n",
      "Example paths from train split:\n",
      "train/img_00000000.png\n",
      "train/img_00000001.png\n",
      "train/img_00000002.png\n",
      "\n",
      "Example paths from validation split:\n",
      "validation/img_00000000.png\n",
      "validation/img_00000001.png\n",
      "validation/img_00000002.png\n",
      "\n",
      "Example paths from test split:\n",
      "test/img_00000000.png\n",
      "test/img_00000001.png\n",
      "test/img_00000002.png\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def process_image_paths(dataset_repo: str, image_dir: str):\n",
    "    \"\"\"\n",
    "    Process a dataset to modify image paths by removing the image_dir prefix\n",
    "    and keep only relative paths.\n",
    "    \n",
    "    Args:\n",
    "        dataset_repo (str): HuggingFace dataset repository name\n",
    "        image_dir (str): Base directory path to remove from image paths\n",
    "    \"\"\"\n",
    "    # Normalize image_dir path\n",
    "    image_dir = os.path.normpath(image_dir)\n",
    "    if not image_dir.endswith(os.sep):\n",
    "        image_dir += os.sep\n",
    "        \n",
    "    # Load dataset\n",
    "    print(f\"Loading dataset from {dataset_repo}\")\n",
    "    dataset = load_dataset(dataset_repo)\n",
    "    \n",
    "    # Process each split\n",
    "    processed_dataset = {}\n",
    "    for split_name, split_data in dataset.items():\n",
    "        print(f\"Processing {split_name} split...\")\n",
    "        \n",
    "        # Get all current paths\n",
    "        image_paths = split_data['image']\n",
    "        \n",
    "        # Process paths to make them relative\n",
    "        new_paths = []\n",
    "        for path in tqdm(image_paths, desc=f\"Processing {split_name} paths\"):\n",
    "            # Normalize path\n",
    "            norm_path = os.path.normpath(path)\n",
    "            # Remove image_dir prefix and convert to forward slashes\n",
    "            relative_path = norm_path.replace(image_dir, '').replace('\\\\', '/')\n",
    "            new_paths.append(relative_path)\n",
    "        \n",
    "        # Create new dataset with modified paths\n",
    "        new_data = {\n",
    "            'image': new_paths,\n",
    "            'latex': split_data['latex']\n",
    "        }\n",
    "        \n",
    "        processed_dataset[split_name] = Dataset.from_dict(new_data)\n",
    "    \n",
    "    # Create new dataset dictionary\n",
    "    final_dataset = DatasetDict(processed_dataset)\n",
    "    \n",
    "    # Push to HuggingFace\n",
    "    try:\n",
    "        print(f\"Pushing processed dataset to anindya-hf-2002/pix2tex_data\")\n",
    "        final_dataset.push_to_hub(f\"anindya-hf-2002/pix2tex_data\")\n",
    "        print(\"Successfully pushed to HuggingFace!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error pushing to hub: {e}\")\n",
    "        # Save locally as fallback\n",
    "        final_dataset.save_to_disk(\"processed_dataset\")\n",
    "        print(\"Dataset saved locally as 'processed_dataset'\")\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_REPO = \"anindya-hf-2002/pix2tex_processed\"\n",
    "    IMAGE_DIR = \"/teamspace/studios/this_studio/Pix2Tex/data\"\n",
    "    \n",
    "    try:\n",
    "        dataset = process_image_paths(DATASET_REPO, IMAGE_DIR)\n",
    "        \n",
    "        # Verify a few examples\n",
    "        for split_name, split_data in dataset.items():\n",
    "            print(f\"\\nExample paths from {split_name} split:\")\n",
    "            for i in range(min(3, len(split_data))):\n",
    "                print(split_data[i]['image'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
