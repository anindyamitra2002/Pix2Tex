{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.27.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage - Memory usage: 0.17 GB\n",
      "\n",
      "Processing dataset 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433842826efb4aef83e18bdbddd4eef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/76266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing dataset 1 - Memory usage: 10.49 GB\n",
      "\n",
      "Processing dataset 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb2d3f8c77d49bf813932e488659473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/552340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing dataset 2 - Memory usage: 59.33 GB\n",
      "\n",
      "Combining datasets...\n",
      "After combining datasets - Memory usage: 59.33 GB\n",
      "\n",
      "Creating and processing splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27321cd894344a91aeaf69a6da6df3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After creating splits - Memory usage: 63.56 GB\n",
      "\n",
      "Pushing to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c002d353e8e04b80b32342959e8e3233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a43491cc864b1a8f8c6e9b9dcf5357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfb0199554a458bba7cbf42284e703f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/943 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b6288ed0f14c508424e20b87173b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f77f49fe0748d3ada48e3c4535e374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/943 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcadfeb5c8644af2b90b0571b3e6be2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784849803353468ab33ba247e0c982e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/943 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe7e49f6f754dbe86aaed2f98995af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0e039def78424cb81a3f4cadcf396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/943 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e263df8b7c4929878a8c9c1f532e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bd797a50ab42bd837fc666b2c053f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e4372fa12a4de0b64bbc1b903324b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003d1086daf44a2bb26c4f3ee0f63999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3227f52bd26430d8b70a7975b25ca78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b70bcaa1c747a8a2748b2087d1ca08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff3c2048caa4f4c9a0ce060e982aba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c4c61ba94844de9d49f59dcceebd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40db2fb49514476b84af1f4a79295df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7c7b06929b4f88a3205222b82b5e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/629 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pushing to hub - Memory usage: 64.46 GB\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024 / 1024  # Convert bytes to GB\n",
    "\n",
    "def log_memory(message):\n",
    "    \"\"\"Log memory usage with a message\"\"\"\n",
    "    print(f\"{message} - Memory usage: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "def process_dataset(dataset, column_name, chunk_size=1000):\n",
    "    \"\"\"Process a single dataset in chunks with progress bar\"\"\"\n",
    "    total_len = len(dataset)\n",
    "    all_images = []\n",
    "    all_latex = []\n",
    "    \n",
    "    # Create chunks of indices\n",
    "    chunks = range(0, total_len, chunk_size)\n",
    "    \n",
    "    # Process chunks with progress bar\n",
    "    with tqdm(total=total_len, desc=\"Processing dataset\") as pbar:\n",
    "        for i in chunks:\n",
    "            # Get chunk indices\n",
    "            end_idx = min(i + chunk_size, total_len)\n",
    "            chunk = dataset.select(range(i, end_idx))\n",
    "            \n",
    "            # Append data\n",
    "            all_images.extend(chunk['image'])\n",
    "            all_latex.extend(chunk[column_name])\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - i)\n",
    "            \n",
    "            # Clear memory\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "    \n",
    "    return all_images, all_latex\n",
    "\n",
    "def create_splits(images, latex, train_size=0.6, val_size=0.2, test_size=0.2):\n",
    "    \"\"\"Create dataset splits with progress bar\"\"\"\n",
    "    total_samples = len(images)\n",
    "    \n",
    "    with tqdm(total=3, desc=\"Creating splits\") as pbar:\n",
    "        # Create train/test split\n",
    "        train_val_size = train_size + val_size\n",
    "        train_val_images, test_images, train_val_latex, test_latex = train_test_split(\n",
    "            images, latex, test_size=test_size, random_state=42\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Create train/val split\n",
    "        val_size_adjusted = val_size / train_val_size\n",
    "        train_images, val_images, train_latex, val_latex = train_test_split(\n",
    "            train_val_images, train_val_latex, test_size=val_size_adjusted, random_state=42\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Create datasets\n",
    "        splits = DatasetDict({\n",
    "            'train': Dataset.from_dict({'image': train_images, 'latex': train_latex}),\n",
    "            'validation': Dataset.from_dict({'image': val_images, 'latex': val_latex}),\n",
    "            'test': Dataset.from_dict({'image': test_images, 'latex': test_latex})\n",
    "        })\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def main():\n",
    "    chunk_size = 20000  # Adjust based on your memory constraints\n",
    "    \n",
    "    log_memory(\"Initial memory usage\")\n",
    "    \n",
    "    # Load and process first dataset\n",
    "    print(\"\\nProcessing dataset 1...\")\n",
    "    dataset1 = load_dataset(\"linxy/LaTeX_OCR\", name=\"synthetic_handwrite\", split=\"train\")\n",
    "    images1, latex1 = process_dataset(dataset1, 'text', chunk_size)\n",
    "    del dataset1\n",
    "    gc.collect()\n",
    "    log_memory(\"After processing dataset 1\")\n",
    "    \n",
    "    # Load and process second dataset\n",
    "    print(\"\\nProcessing dataset 2...\")\n",
    "    dataset2 = load_dataset(\"OleehyO/latex-formulas\", \"cleaned_formulas\", split=\"train\")\n",
    "    images2, latex2 = process_dataset(dataset2, 'latex_formula', chunk_size)\n",
    "    del dataset2\n",
    "    gc.collect()\n",
    "    log_memory(\"After processing dataset 2\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    print(\"\\nCombining datasets...\")\n",
    "    all_images = images1 + images2\n",
    "    all_latex = latex1 + latex2\n",
    "    del images1, images2, latex1, latex2\n",
    "    gc.collect()\n",
    "    log_memory(\"After combining datasets\")\n",
    "    \n",
    "    # Create splits\n",
    "    print(\"\\nCreating and processing splits...\")\n",
    "    splits = create_splits(all_images, all_latex)\n",
    "    log_memory(\"After creating splits\")\n",
    "    \n",
    "    # Push to hub with progress tracking\n",
    "    print(\"\\nPushing to Hugging Face Hub...\")\n",
    "    splits.push_to_hub(\"anindya-hf-2002/pix2tex\", private=True)\n",
    "    log_memory(\"After pushing to hub\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
